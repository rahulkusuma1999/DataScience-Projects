# -*- coding: utf-8 -*-
"""costarican.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CWeKMVyswjm3SLa1JSqSYJ5_WuSCffaN
"""

import pandas as pd
import numpy as np

train=pd.read_csv('sample_data/train.csv')
test=pd.read_csv('sample_data/test.csv')

train.head()
test.head()
train.info()
test.info()



total = train.isnull().sum().sort_values(ascending=False)
percent = (train.isnull().sum()/train.isnull().count()).sort_values(ascending=False)
missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])
missing_data.head(20)



train.drop('rez_esc',inplace=True,axis=1)
 train.drop('v18q1',inplace=True,axis=1)

total = train.isnull().sum().sort_values(ascending=False)
percent = (train.isnull().sum()/train.isnull().count()).sort_values(ascending=False)
missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])
missing_data.head(20)


total = test.isnull().sum().sort_values(ascending=False)
percent = (test.isnull().sum()/test.isnull().count()).sort_values(ascending=False)
missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])
missing_data.head(20)


train.drop('rez_esc',inplace=True,axis=1)
train.drop('v18q1',inplace=True,axis=1)
test.drop('v18q1',inplace=True,axis=1)
test.drop('rez_esc',inplace=True,axis=1)
train.drop(['tamhog','tamviv'],axis=1,inplace=True)
test.drop(['tamhog','tamviv'],axis=1,inplace=True)
train.drop(['meaneduc'],axis=1,inplace=True)
test.drop(['meaneduc'],axis=1,inplace=True)
train.drop(['idhogar'],axis=1,inplace=True)
test.drop(['idhogar'],axis=1,inplace=True)
train.drop('rez_esc',inplace=True,axis=1)
train.drop('v18q1',inplace=True,axis=1)
train.drop(['hhsize'],axis=1,inplace=True)
test.drop(['hhsize'],axis=1,inplace=True)







import seaborn as sns

sns.countplot(train['rooms'])

"""most columns have 2 unique values"""

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
# %matplotlib inline
train.select_dtypes(np.int64).nunique().value_counts().sort_index().plot.bar(color = 'blue', 
                                                                             figsize = (8, 6),
                                                                            edgecolor = 'k', linewidth = 2);
plt.xlabel('Number of Unique Values'); plt.ylabel('Count');
plt.title('Count of Unique Values in Integer Columns');

from collections import OrderedDict

plt.figure(figsize = (20, 16))
plt.style.use('fivethirtyeight')

# Color mapping
colors = OrderedDict({1: 'red', 2: 'orange', 3: 'blue', 4: 'green'})
poverty_mapping = OrderedDict({1: 'extreme', 2: 'moderate', 3: 'vulnerable', 4: 'non vulnerable'})

# Iterate through the float columns
for i, col in enumerate(train.select_dtypes('float')):
    ax = plt.subplot(4, 2, i + 1)
    # Iterate through the poverty levels
    for poverty_level, color in colors.items():
        # Plot each poverty level as a separate line
        sns.kdeplot(train.loc[train['Target'] == poverty_level, col].dropna(), 
                    ax = ax, color = color, label = poverty_mapping[poverty_level])
        
    plt.title(f'{col.capitalize()} Distribution'); plt.xlabel(f'{col}'); plt.ylabel('Density')

plt.subplots_adjust(top = 2)

train.select_dtypes('object').head()

sns.countplot(train['qmobilephone'])

sns.boxplot(train['v2a1'])

train = train.query('v2a1 < 750000')

sns.boxplot(train['v2a1'])

train.isnull().sum()

sns.countplot(train['hhsize'])



sns.countplot(train['escolari'])



# Commented out IPython magic to ensure Python compatibility.
t1=train.head(20)
# %matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sns
cor=train.corr()
plt.subplots(figsize=(12,9))
sns.heatmap(cor)



"""Squared Variables
First, the easiest step: we'll remove all of the squared variables. 
Sometimes variables are squared or transformed as part of feature engineering because it can help linear models learn relationships that are non-linear.
However, since we will be using more complex models, these squared features are redundant. 
They are highly correlated with the non-squared version, and hence can actually hurt our model by adding irrelevant information and also slowing down training.

For an example, let's take a look at SQBage vs age.

sns.lmplot('age', 'SQBage', data = data, fit_reg=False);

plt.title('Squared Age versus Age');
"""

train.drop(['SQBescolari', 'SQBage', 'SQBhogar_total', 'SQBedjefe', 
        'SQBhogar_nin', 'SQBovercrowding', 'SQBdependency', 'SQBmeaned', 'agesq'],axis=1,inplace=True)
test.drop(['SQBescolari', 'SQBage', 'SQBhogar_total', 'SQBedjefe',
        'SQBhogar_nin', 'SQBovercrowding', 'SQBdependency', 'SQBmeaned', 'agesq'],axis=1,inplace=True)

# Commented out IPython magic to ensure Python compatibility.

from fastai.tabular import *

train.head()

train.drop(['idhogar'],axis=1,inplace=True)
test.drop(['idhogar'],axis=1,inplace=True)
ids=test['Id']

train.shape

test.shape

mapping = {"yes": 1, "no": 0}
# Apply same operation to both train and test
for df in [train, test]:
    # Fill in the values with the correct mapping
    df['dependency'] = df['dependency'].replace(mapping).astype(np.float64)
    df['edjefa'] = df['edjefa'].replace(mapping).astype(np.float64)
    df['edjefe'] = df['edjefe'].replace(mapping).astype(np.float64)




Y_train=train['Target']

train.drop('Target',axis=1,inplace=True)

test.drop('Id',axis=1,inplace=True)

train.drop('Id',axis=1,inplace=True)

test['v2a1']=test['v2a1'].fillna(test['v2a1'].mean())

from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()



df_train_scaled = pd.DataFrame(scaler.fit_transform(train), columns=train.columns)
df_test_scaled = pd.DataFrame(scaler.fit_transform(test), columns=test.columns)
print (df_train_scaled.shape)
print (df_test_scaled.shape)
df_test_scaled=df_test_scaled.values



#from sklearn.model_selection import train_test_split
#X_train, X_test, y_train, y_test = train_test_split(, y, test_size = 0.2, random_state = 0)

#from keras.models import Sequential
#from keras.layers import Dense

#classifier=Sequential()
#classifier.add(Dense(output_dim=110,init='uniform',activation='relu',input_dim=125))
#classifier.add(Dense(output_dim=75,init='uniform',activation='relu'))
#classifier.add(Dense(output_dim=45,init='uniform',activation='relu'))
#classifier.add(Dense(output_dim=10,init='uniform',activation='relu'))
#classifier.add(Dense(output_dim=6,init='uniform',activation='relu'))
#classifier.add(Dense(output_dim=1,init='uniform',activation='softmax'))
#classifier.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])
#classifier.fit(df_train_scaled,Y_train, batch_size=20,epochs=100)

'''
import matplotlib.pyplot as plt
import sys,os
import numpy as np
from sklearn import datasets, preprocessing, metrics
from sklearn.decomposition import PCA
import pandas as pd
from keras.models import Sequential
from keras.layers.core import Dense, Activation, Dropout
from keras.wrappers.scikit_learn import KerasClassifier
from keras.utils import np_utils
from sklearn.preprocessing import LabelEncoder
from keras.callbacks import ModelCheckpoint
from sklearn.pipeline import Pipeline
from keras.callbacks import EarlyStopping 
from keras.layers.advanced_activations import PReLU
from keras.layers.normalization import BatchNormalization
from keras.optimizers import SGD,Adam,Adamax,Nadam,Adadelta
from keras.callbacks import ReduceLROnPlateau
from sklearn.preprocessing import MinMaxScaler
#from keras.regularizers import l1l2
from sklearn.preprocessing import LabelEncoder
'''

'''
model = Sequential()
#Base Model
model.add(Dense(64, input_dim=125, init='uniform', activation='relu'))
model.add(Dropout(0.20))
model.add(BatchNormalization(moving_mean_initializer='zeros',momentum=0.9))
model.add(Dense(32 ,init='uniform', activation='relu'))
model.add(BatchNormalization(moving_mean_initializer='zeros',momentum=0.9))
model.add(Dropout(0.20))
#model.add(Dense(128, init='uniform', activation='relu'))
#model.add(BatchNormalization(moving_mean_initializer='zeros',momentum=0.9))
#model.add(Dropout(0.20))
model.add(Dense(1, init='uniform', activation='softmax'))
adam=Adam(lr=1e-3, decay=1e-6)

model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])

filepath="weights.best.hdf5"
checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=0, save_best_only=True, mode='max')
callbacks = [EarlyStopping(monitor='val_loss', patience=3, verbose=0),checkpoint]   
'''



#history=model.fit(train,Y_train,batch_size=100, epochs=50,shuffle=True,callbacks=callbacks)

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV

clf = RandomForestClassifier()
params={'n_estimators': list(range(40,61, 1))}
gs = GridSearchCV(clf, params, cv=5)

gs.fit(df_train_scaled, Y_train)

test.isnull().sum()

preds=gs.predict(df_test_scaled)



submit=pd.DataFrame({'Id': ids, 'Target': preds})

submit.to_csv('submit1.csv', index=False)

